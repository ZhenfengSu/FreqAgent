"""
Agent è½¨è¿¹é¢‘è°±åˆ†ææ¡†æ¶ (Spectral Analysis Framework for Agents)
åˆ†æ Think, Tool Call, Tool Response çš„é¢‘è°±ç‰¹æ€§
"""

import json
import numpy as np
import matplotlib.pyplot as plt
from scipy.fft import dct, fft, fftfreq
from scipy.signal import welch
from typing import List, Dict, Tuple, Optional
import re
from dataclasses import dataclass
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class SegmentType(Enum):
    THINK = "think"
    TOOL_CALL = "tool_call"
    TOOL_RESPONSE = "tool_response"
    SYSTEM = "system"
    USER = "user"
    OTHER = "other"

@dataclass
class Segment:
    """è¡¨ç¤ºä¸€ä¸ªè½¨è¿¹ç‰‡æ®µ"""
    type: SegmentType
    content: str
    start_idx: int
    end_idx: int
    token_count: int

@dataclass
class SpectralMetrics:
    """é¢‘è°±åˆ†ææŒ‡æ ‡"""
    spectral_entropy: float
    high_freq_ratio: float
    energy_concentration: float
    dominant_frequency: float
    total_energy: float

class AgentTrajectoryAnalyzer:
    """Agent è½¨è¿¹åˆ†æå™¨"""
    
    def __init__(self, json_data: dict):
        self.data = json_data
        self.messages = json_data.get('messages', [])
        self.segments: List[Segment] = []
        self.token_embeddings: Optional[np.ndarray] = None
        
    def parse_trajectory(self) -> List[Segment]:
        """è§£æè½¨è¿¹ï¼Œåˆ†å‰²æˆä¸åŒç±»å‹çš„ç‰‡æ®µ"""
        segments = []
        current_idx = 0
        
        for msg in self.messages:
            role = msg.get('role', '')
            content = msg.get('content', '')
            
            if role == 'system':
                segments.append(Segment(
                    type=SegmentType.SYSTEM,
                    content=content,
                    start_idx=current_idx,
                    end_idx=current_idx + len(content),
                    token_count=self._estimate_tokens(content)
                ))
                current_idx += len(content)
                
            elif role == 'assistant':
                # è§£æ assistant æ¶ˆæ¯ä¸­çš„ think, tool_call
                parsed = self._parse_assistant_content(content, current_idx)
                segments.extend(parsed)
                current_idx += len(content)
                
            elif role == 'user':
                # æ£€æŸ¥æ˜¯å¦æ˜¯ tool_response
                if '<tool_response>' in content:
                    segments.append(Segment(
                        type=SegmentType.TOOL_RESPONSE,
                        content=content,
                        start_idx=current_idx,
                        end_idx=current_idx + len(content),
                        token_count=self._estimate_tokens(content)
                    ))
                else:
                    segments.append(Segment(
                        type=SegmentType.USER,
                        content=content,
                        start_idx=current_idx,
                        end_idx=current_idx + len(content),
                        token_count=self._estimate_tokens(content)
                    ))
                current_idx += len(content)
        
        self.segments = segments
        return segments
    
    def _parse_assistant_content(self, content: str, start_idx: int) -> List[Segment]:
        """è§£æ assistant æ¶ˆæ¯ï¼Œåˆ†ç¦» think å’Œ tool_call"""
        segments = []
        current_pos = 0
        
        # åŒ¹é… <think>...</think>
        think_pattern = r'<think>(.*?)</think>'
        # åŒ¹é… <tool_call>...</tool_call>
        tool_call_pattern = r'<tool_call>(.*?)</tool_call>'
        # åŒ¹é… <answer>...</answer>
        answer_pattern = r'<answer>(.*?)</answer>'
        
        # æ‰¾åˆ°æ‰€æœ‰ think å—
        for match in re.finditer(think_pattern, content, re.DOTALL):
            segments.append(Segment(
                type=SegmentType.THINK,
                content=match.group(1),
                start_idx=start_idx + match.start(),
                end_idx=start_idx + match.end(),
                token_count=self._estimate_tokens(match.group(1))
            ))
        
        # æ‰¾åˆ°æ‰€æœ‰ tool_call å—
        for match in re.finditer(tool_call_pattern, content, re.DOTALL):
            segments.append(Segment(
                type=SegmentType.TOOL_CALL,
                content=match.group(1),
                start_idx=start_idx + match.start(),
                end_idx=start_idx + match.end(),
                token_count=self._estimate_tokens(match.group(1))
            ))
        
        return segments
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®— token æ•°é‡ï¼ˆç®€åŒ–ç‰ˆï¼Œçº¦ 4 å­—ç¬¦/tokenï¼‰"""
        return max(1, len(text) // 4)
    
    def generate_synthetic_embeddings(self, dim: int = 256) -> np.ndarray:
        """
        ç”Ÿæˆæ¨¡æ‹Ÿçš„ embedding åºåˆ—
        åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™åº”è¯¥ä»æ¨¡å‹çš„ hidden states æˆ– KV cache ä¸­æå–
        """
        if not self.segments:
            self.parse_trajectory()
        
        all_embeddings = []
        
        for segment in self.segments:
            n_tokens = segment.token_count
            
            if segment.type == SegmentType.THINK:
                # Think: é«˜é¢‘å™ªå£° + ä½é¢‘åŸºå‡†
                low_freq = np.sin(np.linspace(0, 2*np.pi, n_tokens))[:, None] * np.random.randn(dim)
                high_freq = np.random.randn(n_tokens, dim) * 0.5
                embeddings = low_freq + high_freq + np.random.randn(dim) * 0.1  # DC component
                
            elif segment.type == SegmentType.TOOL_CALL:
                # Tool Call: è„‰å†²ä¿¡å·ï¼Œç»“æ„åŒ–
                base = np.zeros((n_tokens, dim))
                # æ·»åŠ è„‰å†²ç‰¹æ€§
                for i in range(n_tokens):
                    base[i] = np.sin(np.arange(dim) * (i + 1) * 0.1) * 2
                embeddings = base + np.random.randn(n_tokens, dim) * 0.1
                
            elif segment.type == SegmentType.TOOL_RESPONSE:
                # Tool Response: æ··åˆä¿¡å·
                structured = np.tile(np.sin(np.linspace(0, 4*np.pi, dim)), (n_tokens, 1))
                noise = np.random.randn(n_tokens, dim) * 0.8
                embeddings = structured + noise
                
            else:
                # å…¶ä»–ç±»å‹
                embeddings = np.random.randn(n_tokens, dim) * 0.3
            
            all_embeddings.append(embeddings)
        
        self.token_embeddings = np.vstack(all_embeddings) if all_embeddings else np.array([])
        return self.token_embeddings


class SpectralAnalyzer:
    """é¢‘è°±åˆ†æå™¨"""
    
    def __init__(self, cutoff_ratio: float = 0.2):
        self.cutoff_ratio = cutoff_ratio
    
    def compute_dct_spectrum(self, signal: np.ndarray) -> np.ndarray:
        """è®¡ç®— DCT é¢‘è°±"""
        if len(signal) < 2:
            return np.array([0])
        return dct(signal, norm='ortho')
    
    def compute_fft_spectrum(self, signal: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """è®¡ç®— FFT é¢‘è°±"""
        if len(signal) < 2:
            return np.array([0]), np.array([0])
        spectrum = np.abs(fft(signal))
        freqs = fftfreq(len(signal))
        return freqs[:len(freqs)//2], spectrum[:len(spectrum)//2]
    
    def compute_power_spectral_density(self, signal: np.ndarray) -> np.ndarray:
        """è®¡ç®—åŠŸç‡è°±å¯†åº¦"""
        if len(signal) < 2:
            return np.array([1])
        spectrum = self.compute_dct_spectrum(signal)
        return np.abs(spectrum) ** 2
    
    def compute_spectral_entropy(self, psd: np.ndarray) -> float:
        """è®¡ç®—è°±ç†µ"""
        psd = psd + 1e-10  # é¿å… log(0)
        p = psd / np.sum(psd)
        return -np.sum(p * np.log2(p))
    
    def compute_high_freq_ratio(self, psd: np.ndarray) -> float:
        """è®¡ç®—é«˜é¢‘èƒ½é‡å æ¯”"""
        if len(psd) < 2:
            return 0
        cutoff = int(len(psd) * (1 - self.cutoff_ratio))
        high_freq_energy = np.sum(psd[cutoff:])
        total_energy = np.sum(psd)
        return high_freq_energy / (total_energy + 1e-10)
    
    def compute_energy_concentration(self, psd: np.ndarray, top_k: int = 5) -> float:
        """è®¡ç®—èƒ½é‡é›†ä¸­åº¦ï¼ˆå‰ top_k ä¸ªé¢‘ç‡åˆ†é‡çš„èƒ½é‡å æ¯”ï¼‰"""
        if len(psd) < top_k:
            return 1.0
        sorted_psd = np.sort(psd)[::-1]
        top_energy = np.sum(sorted_psd[:top_k])
        total_energy = np.sum(psd)
        return top_energy / (total_energy + 1e-10)
    
    def analyze_segment(self, embeddings: np.ndarray) -> SpectralMetrics:
        """åˆ†æå•ä¸ªç‰‡æ®µçš„é¢‘è°±ç‰¹æ€§"""
        if embeddings.size == 0 or len(embeddings) < 2:
            return SpectralMetrics(0, 0, 0, 0, 0)
        
        # å¯¹æ¯ä¸ªç»´åº¦è®¡ç®— PSDï¼Œç„¶åå¹³å‡
        n_tokens, n_dims = embeddings.shape
        all_psd = []
        
        for d in range(n_dims):
            psd = self.compute_power_spectral_density(embeddings[:, d])
            all_psd.append(psd)
        
        # å¯¹é½å¹¶å¹³å‡
        min_len = min(len(p) for p in all_psd)
        aligned_psd = np.array([p[:min_len] for p in all_psd])
        avg_psd = np.mean(aligned_psd, axis=0)
        
        return SpectralMetrics(
            spectral_entropy=self.compute_spectral_entropy(avg_psd),
            high_freq_ratio=self.compute_high_freq_ratio(avg_psd),
            energy_concentration=self.compute_energy_concentration(avg_psd),
            dominant_frequency=np.argmax(avg_psd) / len(avg_psd),
            total_energy=np.sum(avg_psd)
        )


class TrajectoryVisualizer:
    """è½¨è¿¹å¯è§†åŒ–å™¨"""
    
    def __init__(self, analyzer: AgentTrajectoryAnalyzer, spectral: SpectralAnalyzer):
        self.analyzer = analyzer
        self.spectral = spectral
        
    def plot_segment_statistics(self, save_path: str = None):
        """ç»˜åˆ¶ç‰‡æ®µç»Ÿè®¡ä¿¡æ¯"""
        segments = self.analyzer.segments
        
        # ç»Ÿè®¡å„ç±»å‹ç‰‡æ®µ
        type_counts = {}
        type_tokens = {}
        for seg in segments:
            t = seg.type.value
            type_counts[t] = type_counts.get(t, 0) + 1
            type_tokens[t] = type_tokens.get(t, 0) + seg.token_count
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # ç‰‡æ®µæ•°é‡
        types = list(type_counts.keys())
        counts = [type_counts[t] for t in types]
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
        
        axes[0].bar(types, counts, color=colors[:len(types)])
        axes[0].set_title('Segment Count by Type', fontsize=14)
        axes[0].set_xlabel('Segment Type')
        axes[0].set_ylabel('Count')
        axes[0].tick_params(axis='x', rotation=45)
        
        # Token æ•°é‡
        tokens = [type_tokens[t] for t in types]
        axes[1].bar(types, tokens, color=colors[:len(types)])
        axes[1].set_title('Token Count by Type', fontsize=14)
        axes[1].set_xlabel('Segment Type')
        axes[1].set_ylabel('Token Count')
        axes[1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()
        
    def plot_spectrogram(self, save_path: str = None):
        """ç»˜åˆ¶é¢‘è°±çƒ­åŠ›å›¾"""
        if self.analyzer.token_embeddings is None:
            self.analyzer.generate_synthetic_embeddings()
        
        embeddings = self.analyzer.token_embeddings
        if embeddings.size == 0:
            print("No embeddings to visualize")
            return
            
        n_tokens, n_dims = embeddings.shape
        
        # è®¡ç®—æ¯ä¸ªä½ç½®çš„é¢‘è°±ï¼ˆä½¿ç”¨æ»‘åŠ¨çª—å£ï¼‰
        window_size = min(32, n_tokens // 4)
        if window_size < 4:
            window_size = 4
            
        hop_size = max(1, window_size // 2)
        
        spectrograms = []
        positions = []
        
        for i in range(0, n_tokens - window_size, hop_size):
            window = embeddings[i:i+window_size]
            # å¯¹æ‰€æœ‰ç»´åº¦å¹³å‡
            avg_signal = np.mean(window, axis=1)
            spectrum = np.abs(dct(avg_signal, norm='ortho'))
            spectrograms.append(spectrum)
            positions.append(i + window_size // 2)
        
        if not spectrograms:
            print("Not enough data for spectrogram")
            return
            
        spectrogram = np.array(spectrograms).T
        
        fig, ax = plt.subplots(figsize=(16, 6))
        
        im = ax.imshow(spectrogram, aspect='auto', cmap='viridis',
                       extent=[0, n_tokens, window_size//2, 0])
        
        # æ ‡æ³¨ç‰‡æ®µè¾¹ç•Œ
        colors_map = {
            SegmentType.THINK: 'red',
            SegmentType.TOOL_CALL: 'yellow',
            SegmentType.TOOL_RESPONSE: 'cyan'
        }
        
        current_pos = 0
        for seg in self.analyzer.segments:
            if seg.type in colors_map:
                ax.axvline(x=current_pos, color=colors_map[seg.type], 
                          linestyle='--', alpha=0.7, linewidth=1.5)
            current_pos += seg.token_count
        
        plt.colorbar(im, ax=ax, label='Energy')
        ax.set_xlabel('Token Position', fontsize=12)
        ax.set_ylabel('Frequency (DCT coefficient index)', fontsize=12)
        ax.set_title('Spectrogram of Agent Trajectory\n(Red: Think, Yellow: Tool Call, Cyan: Tool Response)', 
                    fontsize=14)
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()
        
    def plot_energy_decay_curves(self, save_path: str = None):
        """ç»˜åˆ¶ä¸åŒç±»å‹ç‰‡æ®µçš„èƒ½é‡è¡°å‡æ›²çº¿"""
        if self.analyzer.token_embeddings is None:
            self.analyzer.generate_synthetic_embeddings()
        
        embeddings = self.analyzer.token_embeddings
        segments = self.analyzer.segments
        
        # æ”¶é›†å„ç±»å‹çš„ PSD
        type_psds = {
            SegmentType.THINK: [],
            SegmentType.TOOL_CALL: [],
            SegmentType.TOOL_RESPONSE: []
        }
        
        current_pos = 0
        for seg in segments:
            if seg.type in type_psds and seg.token_count >= 4:
                seg_embeddings = embeddings[current_pos:current_pos + seg.token_count]
                if len(seg_embeddings) >= 4:
                    avg_signal = np.mean(seg_embeddings, axis=1)
                    psd = self.spectral.compute_power_spectral_density(avg_signal)
                    # å½’ä¸€åŒ–
                    psd = psd / (np.max(psd) + 1e-10)
                    type_psds[seg.type].append(psd)
            current_pos += seg.token_count
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        colors = {
            SegmentType.THINK: '#FF6B6B',
            SegmentType.TOOL_CALL: '#4ECDC4',
            SegmentType.TOOL_RESPONSE: '#45B7D1'
        }
        
        for seg_type, psds in type_psds.items():
            if psds:
                # å¯¹é½å¹¶å¹³å‡
                min_len = min(len(p) for p in psds)
                aligned = np.array([p[:min_len] for p in psds])
                avg_psd = np.mean(aligned, axis=0)
                std_psd = np.std(aligned, axis=0)
                
                freqs = np.arange(len(avg_psd)) / len(avg_psd)
                ax.plot(freqs, avg_psd, label=seg_type.value, 
                       color=colors[seg_type], linewidth=2)
                ax.fill_between(freqs, avg_psd - std_psd, avg_psd + std_psd,
                               color=colors[seg_type], alpha=0.2)
        
        ax.set_xlabel('Normalized Frequency', fontsize=12)
        ax.set_ylabel('Normalized Power Spectral Density', fontsize=12)
        ax.set_title('Energy Decay Curves by Segment Type', fontsize=14)
        ax.legend(fontsize=11)
        ax.set_xlim([0, 0.5])
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()
        
    def plot_spectral_metrics_comparison(self, save_path: str = None):
        """ç»˜åˆ¶é¢‘è°±æŒ‡æ ‡å¯¹æ¯”"""
        if self.analyzer.token_embeddings is None:
            self.analyzer.generate_synthetic_embeddings()
        
        embeddings = self.analyzer.token_embeddings
        segments = self.analyzer.segments
        
        # æ”¶é›†å„ç±»å‹çš„æŒ‡æ ‡
        type_metrics = {
            SegmentType.THINK: {'entropy': [], 'hfr': [], 'concentration': []},
            SegmentType.TOOL_CALL: {'entropy': [], 'hfr': [], 'concentration': []},
            SegmentType.TOOL_RESPONSE: {'entropy': [], 'hfr': [], 'concentration': []}
        }
        
        current_pos = 0
        for seg in segments:
            if seg.type in type_metrics and seg.token_count >= 4:
                seg_embeddings = embeddings[current_pos:current_pos + seg.token_count]
                if len(seg_embeddings) >= 4:
                    metrics = self.spectral.analyze_segment(seg_embeddings)
                    type_metrics[seg.type]['entropy'].append(metrics.spectral_entropy)
                    type_metrics[seg.type]['hfr'].append(metrics.high_freq_ratio)
                    type_metrics[seg.type]['concentration'].append(metrics.energy_concentration)
            current_pos += seg.token_count
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        metric_names = ['Spectral Entropy', 'High-Freq Ratio', 'Energy Concentration']
        metric_keys = ['entropy', 'hfr', 'concentration']
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        
        for idx, (name, key) in enumerate(zip(metric_names, metric_keys)):
            data = []
            labels = []
            for seg_type in [SegmentType.THINK, SegmentType.TOOL_CALL, SegmentType.TOOL_RESPONSE]:
                if type_metrics[seg_type][key]:
                    data.append(type_metrics[seg_type][key])
                    labels.append(seg_type.value)
            
            if data:
                bp = axes[idx].boxplot(data, labels=labels, patch_artist=True)
                for patch, color in zip(bp['boxes'], colors[:len(data)]):
                    patch.set_facecolor(color)
                    patch.set_alpha(0.7)
            
            axes[idx].set_title(name, fontsize=12)
            axes[idx].set_ylabel('Value')
            axes[idx].tick_params(axis='x', rotation=30)
            axes[idx].grid(True, alpha=0.3)
        
        plt.suptitle('Spectral Metrics Comparison by Segment Type', fontsize=14, y=1.02)
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()


def analyze_trajectory_file(json_data: dict) -> dict:
    """åˆ†æå•ä¸ªè½¨è¿¹æ–‡ä»¶"""
    # åˆå§‹åŒ–åˆ†æå™¨
    trajectory_analyzer = AgentTrajectoryAnalyzer(json_data)
    segments = trajectory_analyzer.parse_trajectory()
    trajectory_analyzer.generate_synthetic_embeddings()
    
    spectral_analyzer = SpectralAnalyzer(cutoff_ratio=0.2)
    
    # è®¡ç®—å„ç±»å‹ç‰‡æ®µçš„é¢‘è°±æŒ‡æ ‡
    results = {
        'segment_counts': {},
        'token_counts': {},
        'spectral_metrics': {}
    }
    
    embeddings = trajectory_analyzer.token_embeddings
    current_pos = 0
    
    type_metrics = {t: [] for t in SegmentType}
    
    for seg in segments:
        t = seg.type.value
        results['segment_counts'][t] = results['segment_counts'].get(t, 0) + 1
        results['token_counts'][t] = results['token_counts'].get(t, 0) + seg.token_count
        
        if seg.token_count >= 4:
            seg_embeddings = embeddings[current_pos:current_pos + seg.token_count]
            if len(seg_embeddings) >= 4:
                metrics = spectral_analyzer.analyze_segment(seg_embeddings)
                type_metrics[seg.type].append(metrics)
        
        current_pos += seg.token_count
    
    # æ±‡æ€»æŒ‡æ ‡
    for seg_type, metrics_list in type_metrics.items():
        if metrics_list:
            avg_metrics = {
                'avg_spectral_entropy': np.mean([m.spectral_entropy for m in metrics_list]),
                'avg_high_freq_ratio': np.mean([m.high_freq_ratio for m in metrics_list]),
                'avg_energy_concentration': np.mean([m.energy_concentration for m in metrics_list]),
                'avg_total_energy': np.mean([m.total_energy for m in metrics_list]),
                'count': len(metrics_list)
            }
            results['spectral_metrics'][seg_type.value] = avg_metrics
    
    return results, trajectory_analyzer, spectral_analyzer


def main():
    """ä¸»å‡½æ•°"""
    json_data = {
        "question": "I'm researching species that became invasive after people who kept them as pets released them...",
        "answer": "34689",
        "rollout_id": 1,
        "messages": [
            {"role": "system", "content": "You are a Web Information Seeking Master..."},
            {"role": "user", "content": "A conversation between User and Assistant..."},
            {"role": "assistant", "content": "<think>To determine the exact zip codes where the fish species from Finding Nemo became established as nonnative, I need highly specific, authoritative information...</think>\n<tool_call>\n{\"name\": \"search\", \"arguments\": {\"query\": [\"Finding Nemo fish nonnative found zip codes\", \"USGS Finding Nemo fish nonnative USGS\"]}}\n</tool_call>"},
            {"role": "user", "content": "<tool_response>\nA Google search for 'Finding Nemo fish nonnative found zip codes' found 9 results...\n</tool_response>"},
            {"role": "assistant", "content": "<think>The USGS Nonindigenous Aquatic Species database is likely to contain authoritative, georeferenced records...</think>\n<tool_call>\n{\"name\": \"visit\", \"arguments\": {\"url\": [\"https://nas.er.usgs.gov/queries/FactSheet.aspx?speciesID=3243\"], \"goal\": \"Extract the zip codes\"}}\n</tool_call>"},
            {"role": "user", "content": "<tool_response>\nThe useful information in https://nas.er.usgs.gov/queries/FactSheet.aspx?speciesID=3243...\n</tool_response>"},
            {"role": "assistant", "content": "<think>The previous attempts to directly access authoritative USGS fact sheets were unsuccessful...</think>\n<tool_call>\n{\"name\": \"search\", \"arguments\": {\"query\": [\"Australis nonnative zip codes USGS\", \"Finding Nemo fish nonnative zip codes\"]}}\n</tool_call>"},
        ],
        "prediction": "The clown anemonefish was first reported at Fred Howard Park, Florida (zip code 34689).",
        "termination": "generate an answer as token limit reached",
        "tool_call_search_times": 11,
        "tool_call_visit_times": 11
    }
    
    print("=" * 60)
    print("Agent è½¨è¿¹é¢‘è°±åˆ†æå®éªŒ")
    print("=" * 60)
    
    # åˆ†æè½¨è¿¹
    results, trajectory_analyzer, spectral_analyzer = analyze_trajectory_file(json_data)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š ç‰‡æ®µç»Ÿè®¡:")
    print("-" * 40)
    for seg_type, count in results['segment_counts'].items():
        tokens = results['token_counts'].get(seg_type, 0)
        print(f"  {seg_type:15s}: {count:3d} ä¸ªç‰‡æ®µ, {tokens:6d} tokens")
    
    print("\nğŸ“ˆ é¢‘è°±æŒ‡æ ‡:")
    print("-" * 40)
    for seg_type, metrics in results['spectral_metrics'].items():
        print(f"\n  [{seg_type}]")
        print(f"    è°±ç†µ (Spectral Entropy):     {metrics['avg_spectral_entropy']:.4f}")
        print(f"    é«˜é¢‘èƒ½é‡å æ¯” (HFR):          {metrics['avg_high_freq_ratio']:.4f}")
        print(f"    èƒ½é‡é›†ä¸­åº¦ (Concentration):  {metrics['avg_energy_concentration']:.4f}")
        print(f"    å¹³å‡æ€»èƒ½é‡:                  {metrics['avg_total_energy']:.4f}")
    
    # å¯è§†åŒ–
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    visualizer = TrajectoryVisualizer(trajectory_analyzer, spectral_analyzer)
    
    # ç»˜åˆ¶å„ç§å›¾è¡¨
    visualizer.plot_segment_statistics()
    visualizer.plot_spectrogram()
    visualizer.plot_energy_decay_curves()
    visualizer.plot_spectral_metrics_comparison()
    
    
    return results


if __name__ == "__main__":
    results = main()