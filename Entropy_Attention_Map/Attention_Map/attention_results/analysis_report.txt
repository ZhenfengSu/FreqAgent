============================================================
ATTENTION MAP ANALYSIS REPORT
============================================================


========================================
SAMPLE 0
========================================

Tag Spans Found: 8
  - <think>: tokens [28:60]
  - <tool_call>: tokens [60:99]
  - <tool_response>: tokens [104:132]
  - <think>: tokens [137:163]
  - <tool_call>: tokens [163:210]
  - <tool_response>: tokens [215:249]
  - <think>: tokens [254:272]
  - <tool_call>: tokens [272:309]

Cross-Region Attention Summary:

  Layer 8:
    Strongest flow: other -> other (0.0300)
    think self-attention: 0.0056
    tool_call self-attention: 0.0037
    tool_response self-attention: 0.0052

  Layer 16:
    Strongest flow: other -> other (0.0276)
    think self-attention: 0.0107
    tool_call self-attention: 0.0059
    tool_response self-attention: 0.0103

  Layer 24:
    Strongest flow: other -> other (0.0298)
    think self-attention: 0.0114
    tool_call self-attention: 0.0073
    tool_response self-attention: 0.0125

  Layer 31:
    Strongest flow: other -> other (0.0299)
    think self-attention: 0.0093
    tool_call self-attention: 0.0059
    tool_response self-attention: 0.0066

========================================
SAMPLE 1
========================================

Tag Spans Found: 8
  - <think>: tokens [28:60]
  - <tool_call>: tokens [60:99]
  - <tool_response>: tokens [104:132]
  - <think>: tokens [137:163]
  - <tool_call>: tokens [163:210]
  - <tool_response>: tokens [215:249]
  - <think>: tokens [254:272]
  - <tool_call>: tokens [272:309]

Cross-Region Attention Summary:

  Layer 8:
    Strongest flow: other -> other (0.0300)
    think self-attention: 0.0056
    tool_call self-attention: 0.0037
    tool_response self-attention: 0.0052

  Layer 16:
    Strongest flow: other -> other (0.0276)
    think self-attention: 0.0107
    tool_call self-attention: 0.0059
    tool_response self-attention: 0.0103

  Layer 24:
    Strongest flow: other -> other (0.0298)
    think self-attention: 0.0114
    tool_call self-attention: 0.0073
    tool_response self-attention: 0.0125

  Layer 31:
    Strongest flow: other -> other (0.0299)
    think self-attention: 0.0093
    tool_call self-attention: 0.0059
    tool_response self-attention: 0.0066

========================================
SAMPLE 2
========================================

Tag Spans Found: 8
  - <think>: tokens [28:60]
  - <tool_call>: tokens [60:99]
  - <tool_response>: tokens [104:132]
  - <think>: tokens [137:163]
  - <tool_call>: tokens [163:210]
  - <tool_response>: tokens [215:249]
  - <think>: tokens [254:272]
  - <tool_call>: tokens [272:309]

Cross-Region Attention Summary:

  Layer 8:
    Strongest flow: other -> other (0.0300)
    think self-attention: 0.0056
    tool_call self-attention: 0.0037
    tool_response self-attention: 0.0052

  Layer 16:
    Strongest flow: other -> other (0.0276)
    think self-attention: 0.0107
    tool_call self-attention: 0.0059
    tool_response self-attention: 0.0103

  Layer 24:
    Strongest flow: other -> other (0.0298)
    think self-attention: 0.0114
    tool_call self-attention: 0.0073
    tool_response self-attention: 0.0125

  Layer 31:
    Strongest flow: other -> other (0.0299)
    think self-attention: 0.0093
    tool_call self-attention: 0.0059
    tool_response self-attention: 0.0066

========================================
SAMPLE 3
========================================

Tag Spans Found: 8
  - <think>: tokens [28:60]
  - <tool_call>: tokens [60:99]
  - <tool_response>: tokens [104:132]
  - <think>: tokens [137:163]
  - <tool_call>: tokens [163:210]
  - <tool_response>: tokens [215:249]
  - <think>: tokens [254:272]
  - <tool_call>: tokens [272:309]

Cross-Region Attention Summary:

  Layer 8:
    Strongest flow: other -> other (0.0300)
    think self-attention: 0.0056
    tool_call self-attention: 0.0037
    tool_response self-attention: 0.0052

  Layer 16:
    Strongest flow: other -> other (0.0276)
    think self-attention: 0.0107
    tool_call self-attention: 0.0059
    tool_response self-attention: 0.0103

  Layer 24:
    Strongest flow: other -> other (0.0298)
    think self-attention: 0.0114
    tool_call self-attention: 0.0073
    tool_response self-attention: 0.0125

  Layer 31:
    Strongest flow: other -> other (0.0299)
    think self-attention: 0.0093
    tool_call self-attention: 0.0059
    tool_response self-attention: 0.0066