import json
import re
import numpy as np
import requests
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, field
import logging
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.collections import BrokenBarHCollection

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


@dataclass
class PhaseEntropy:
    """å­˜å‚¨æŸä¸ªPhaseçš„ç†µç»Ÿè®¡ç»“æœ"""
    phase_name: str
    entropies: List[float] = field(default_factory=list)
    token_count: int = 0
    
    @property
    def average_entropy(self) -> float:
        if not self.entropies:
            return 0.0
        return float(np.mean(self.entropies))
    
    @property
    def std_entropy(self) -> float:
        if len(self.entropies) < 2:
            return 0.0
        return float(np.std(self.entropies))
    
    def add_entropies(self, entropies: List[float]):
        self.entropies.extend(entropies)
        self.token_count += len(entropies)


class SGLangEntropyAnalyzer:
    """ä½¿ç”¨ sglang API è¿›è¡Œç†µç»Ÿè®¡åˆ†æ"""
    
    def __init__(self, base_url: str = "http://localhost:6001"):
        self.base_url = base_url.rstrip('/')
        self.model_name = self._get_model_name()
        
        # Phaseæ ‡è®°æ­£åˆ™è¡¨è¾¾å¼
        self.phase_patterns = {
            'think': re.compile(r'<think>(.*?)</think>', re.DOTALL),
            'tool_call': re.compile(r'<tool_call>(.*?)</tool_call>', re.DOTALL),
            'tool_response': re.compile(r'<tool_response>(.*?)</tool_response>', re.DOTALL),
            'answer': re.compile(r'<answer>(.*?)</answer>', re.DOTALL),
        }
        
        # Phase é¢œè‰²é…ç½®
        self.phase_colors = {
            'think': '#FF6B6B',        # çº¢è‰²
            'tool_call': '#4ECDC4',    # é’è‰²
            'tool_response': '#45B7D1', # è“è‰²
            'answer': '#96CEB4',        # ç»¿è‰²
        }
        
        logger.info(f"åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {self.model_name}")
    
    def _get_model_name(self) -> str:
        """è·å–æ¨¡å‹åç§°"""
        try:
            response = requests.get(f"{self.base_url}/v1/models", timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data.get('data'):
                    return data['data'][0]['id']
        except Exception as e:
            logger.warning(f"è·å–æ¨¡å‹åç§°å¤±è´¥: {e}")
        return "default"
    
    def calculate_entropy_from_top_logprobs(self, top_logprobs_dict: Dict[str, float]) -> float:
        """ä» top_logprobs å­—å…¸è®¡ç®—ç†µ"""
        if not top_logprobs_dict:
            return 0.0
        
        logprobs = list(top_logprobs_dict.values())
        probs = np.exp(np.array(logprobs, dtype=np.float64))
        
        prob_sum = probs.sum()
        if prob_sum <= 0:
            return 0.0
        probs = probs / prob_sum
        
        entropy = -np.sum(probs * np.log(probs + 1e-10))
        return float(entropy)
    
    def get_completion_with_logprobs(self, prompt: str, max_tokens: int = 1024) -> Optional[Dict]:
        """è·å– completion å’Œ logprobs"""
        url = f"{self.base_url}/v1/completions"
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": 0,
            "logprobs": 10,
        }
        
        try:
            response = requests.post(url, json=payload, timeout=300)
            
            if response.status_code == 200:
                result = response.json()
                return self._parse_response(result)
            else:
                logger.warning(f"APIè¿”å›é”™è¯¯ {response.status_code}: {response.text[:200]}")
                return None
                
        except requests.exceptions.Timeout:
            logger.error("è¯·æ±‚è¶…æ—¶")
            return None
        except Exception as e:
            logger.error(f"è¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def _parse_response(self, result: Dict) -> Optional[Dict]:
        """è§£æ API å“åº”"""
        if 'choices' not in result or len(result['choices']) == 0:
            return None
        
        choice = result['choices'][0]
        logprobs_data = choice.get('logprobs', {})
        
        return {
            'text': choice.get('text', ''),
            'tokens': logprobs_data.get('tokens', []),
            'token_logprobs': logprobs_data.get('token_logprobs', []),
            'top_logprobs': logprobs_data.get('top_logprobs', []),
        }
    
    def compute_token_entropies(self, top_logprobs: List[Dict[str, float]]) -> List[float]:
        """è®¡ç®—æ¯ä¸ª token ä½ç½®çš„ç†µ"""
        entropies = []
        for top_lp in top_logprobs:
            if top_lp is None or not isinstance(top_lp, dict):
                entropies.append(0.0)
                continue
            entropy = self.calculate_entropy_from_top_logprobs(top_lp)
            entropies.append(entropy)
        return entropies
    
    def extract_phases_with_token_ranges(
        self,
        content: str,
        tokens: List[str]
    ) -> Dict[str, List[Tuple[int, int]]]:
        """
        æå–å„ä¸ª phase å¯¹åº”çš„ token ç´¢å¼•èŒƒå›´
        è¿”å›: {phase_name: [(start_token_idx, end_token_idx), ...]}
        """
        phases = {phase: [] for phase in self.phase_patterns.keys()}
        
        if not tokens:
            return phases
        
        # é‡å»º token çš„å­—ç¬¦ä½ç½®æ˜ å°„
        token_char_positions = []  # [(char_start, char_end), ...]
        current_pos = 0
        
        for token in tokens:
            token_start = current_pos
            token_end = current_pos + len(token)
            token_char_positions.append((token_start, token_end))
            current_pos = token_end
        
        # å¯¹æ¯ä¸ª phase patternï¼Œæ‰¾åˆ°å…¶åœ¨ content ä¸­çš„å­—ç¬¦èŒƒå›´
        for phase_name, pattern in self.phase_patterns.items():
            for match in pattern.finditer(content):
                # æ ‡ç­¾å†…éƒ¨å†…å®¹çš„å­—ç¬¦ä½ç½®
                tag_open = f'<{phase_name}>'
                tag_close = f'</{phase_name}>'
                inner_char_start = match.start() + len(tag_open)
                inner_char_end = match.end() - len(tag_close)
                
                # æ‰¾åˆ°å¯¹åº”çš„ token èŒƒå›´
                start_token_idx = None
                end_token_idx = None
                
                for i, (t_start, t_end) in enumerate(token_char_positions):
                    # token ä¸ phase å†…å®¹æœ‰äº¤é›†
                    if t_end > inner_char_start and t_start < inner_char_end:
                        if start_token_idx is None:
                            start_token_idx = i
                        end_token_idx = i + 1
                
                if start_token_idx is not None:
                    phases[phase_name].append((start_token_idx, end_token_idx))
        
        return phases
    
    def build_prompt_for_message(self, messages: List[Dict], target_msg_idx: int) -> str:
        """
        æ„å»ºç”¨äºè®©æ¨¡å‹ç»­å†™ç¬¬ target_msg_idx ä¸ª message çš„ prompt
        """
        prompt_parts = []
        
        for i, msg in enumerate(messages):
            if i >= target_msg_idx:
                break
            
            role = msg.get('role', '')
            content = msg.get('content', '')
            prompt_parts.append(f"<|im_start|>{role}\n{content}<|im_end|>")
        
        # æ·»åŠ ç›®æ ‡ message çš„å¼€å¤´
        target_role = messages[target_msg_idx].get('role', 'assistant')
        prompt_parts.append(f"<|im_start|>{target_role}\n")
        
        return "\n".join(prompt_parts)
    
    def analyze_single_case(self, messages: List[Dict]) -> Dict:
        """
        åˆ†æå•ä¸ª caseï¼Œè¿”å›è¯¦ç»†çš„ token ç†µä¿¡æ¯
        è¿”å›: {
            'tokens': List[str],
            'entropies': List[float],
            'phase_ranges': Dict[str, List[Tuple[int, int]]],
            'generated_text': str
        }
        """
        all_tokens = []
        all_entropies = []
        all_phase_ranges = {phase: [] for phase in self.phase_patterns.keys()}
        all_generated_text = ""
        
        token_offset = 0  # ç”¨äºç´¯è®¡ token ä½ç½®åç§»
        
        for msg_idx, msg in enumerate(messages):
            role = msg.get('role', '')
            content = msg.get('content', '')
            
            if not content or len(content) < 5:
                continue
            
            # æ£€æµ‹è¿™ä¸ª message ä¸­æœ‰å“ªäº› phase
            detected_phases = []
            for phase_name, pattern in self.phase_patterns.items():
                if pattern.search(content):
                    detected_phases.append(phase_name)
            
            if not detected_phases:
                logger.debug(f"Message {msg_idx} (role={role}): æœªæ£€æµ‹åˆ°ä»»ä½• phase æ ‡ç­¾ï¼Œè·³è¿‡")
                continue
            
            logger.info(f"Message {msg_idx} (role={role}): æ£€æµ‹åˆ° phases: {detected_phases}")
            
            # æ„å»º prompt
            prompt = self.build_prompt_for_message(messages, msg_idx)
            
            # ä¼°ç®—éœ€è¦ç”Ÿæˆçš„ token æ•°
            estimated_tokens = int(len(content) * 1.5)
            max_gen_tokens = min(max(estimated_tokens, 512), 8192)
            
            # è·å– logprobs
            result = self.get_completion_with_logprobs(prompt, max_tokens=max_gen_tokens)
            
            if result is None:
                logger.warning(f"è·å– logprobs å¤±è´¥ï¼Œè·³è¿‡ message {msg_idx}")
                continue
            
            tokens = result.get('tokens', [])
            top_logprobs = result.get('top_logprobs', [])
            
            if not tokens or not top_logprobs:
                logger.warning(f"Message {msg_idx}: tokens æˆ– logprobs ä¸ºç©º")
                continue
            
            # è®¡ç®—æ¯ä¸ª token çš„ç†µ
            token_entropies = self.compute_token_entropies(top_logprobs)
            
            # é‡å»ºç”Ÿæˆçš„æ–‡æœ¬
            generated_text = ''.join(tokens)
            
            logger.info(f"Message {msg_idx}: ç”Ÿæˆäº† {len(tokens)} ä¸ª tokens")
            
            # åœ¨ç”Ÿæˆçš„æ–‡æœ¬ä¸­å®šä½ phases
            phase_token_ranges = self.extract_phases_with_token_ranges(generated_text, tokens)
            
            # ç´¯åŠ åˆ°å…¨å±€ç»“æœä¸­ï¼Œè°ƒæ•´ token èŒƒå›´çš„åç§»
            all_tokens.extend(tokens)
            all_entropies.extend(token_entropies)
            all_generated_text += generated_text
            
            for phase_name, ranges in phase_token_ranges.items():
                for start_idx, end_idx in ranges:
                    # åŠ ä¸Šåç§»é‡
                    all_phase_ranges[phase_name].append((start_idx + token_offset, end_idx + token_offset))
            
            token_offset += len(tokens)
        
        return {
            'tokens': all_tokens,
            'entropies': all_entropies,
            'phase_ranges': all_phase_ranges,
            'generated_text': all_generated_text
        }
    
    def plot_entropy_curve(
        self, 
        case_data: Dict, 
        case_idx: int = 0,
        save_path: str = None,
        figsize: Tuple[int, int] = (16, 8),
        show_tokens: bool = False
    ):
        """
        ç»˜åˆ¶å•ä¸ª case çš„ token ç†µå˜åŒ–æ›²çº¿
        
        Args:
            case_data: analyze_single_case è¿”å›çš„æ•°æ®
            case_idx: case ç´¢å¼•ï¼ˆç”¨äºæ ‡é¢˜ï¼‰
            save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™æ˜¾ç¤º
            figsize: å›¾ç‰‡å¤§å°
            show_tokens: æ˜¯å¦åœ¨ x è½´æ˜¾ç¤º token æ–‡æœ¬
        """
        tokens = case_data['tokens']
        entropies = case_data['entropies']
        phase_ranges = case_data['phase_ranges']
        
        if not tokens or not entropies:
            logger.warning("æ²¡æœ‰æ•°æ®å¯ä»¥ç»˜åˆ¶")
            return
        
        fig, ax = plt.subplots(figsize=figsize)
        
        # ç»˜åˆ¶ç†µæ›²çº¿
        x = np.arange(len(entropies))
        ax.plot(x, entropies, 'b-', linewidth=0.8, alpha=0.7, label='Token Entropy')
        ax.scatter(x, entropies, s=10, c='blue', alpha=0.5)
        
        # ç»˜åˆ¶ phase èƒŒæ™¯åŒºåŸŸ
        y_min, y_max = ax.get_ylim()
        if y_max <= y_min:
            y_max = max(entropies) * 1.1 if entropies else 1.0
            y_min = 0
        
        legend_patches = []
        for phase_name, ranges in phase_ranges.items():
            if not ranges:
                continue
            
            color = self.phase_colors.get(phase_name, '#CCCCCC')
            
            for start_idx, end_idx in ranges:
                ax.axvspan(start_idx - 0.5, end_idx - 0.5, 
                          alpha=0.3, color=color, 
                          label=f'{phase_name}' if start_idx == ranges[0][0] else None)
                
                # åœ¨åŒºåŸŸé¡¶éƒ¨æ·»åŠ æ ‡ç­¾
                mid_x = (start_idx + end_idx) / 2
                ax.text(mid_x, y_max * 0.95, phase_name, 
                       ha='center', va='top', fontsize=9, 
                       color=color, fontweight='bold',
                       bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
            
            # æ·»åŠ åˆ°å›¾ä¾‹
            patch = mpatches.Patch(color=color, alpha=0.3, label=phase_name)
            legend_patches.append(patch)
        
        # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
        ax.set_xlabel('Token Position', fontsize=12)
        ax.set_ylabel('Entropy', fontsize=12)
        ax.set_title(f'Token Entropy Curve - Case {case_idx}\n(Total Tokens: {len(tokens)})', fontsize=14)
        
        # æ·»åŠ å›¾ä¾‹
        if legend_patches:
            ax.legend(handles=legend_patches, loc='upper right', fontsize=10)
        
        # æ·»åŠ ç½‘æ ¼
        ax.grid(True, linestyle='--', alpha=0.3)
        
        # è®¾ç½® x è½´èŒƒå›´
        ax.set_xlim(-0.5, len(entropies) - 0.5)
        
        # å¦‚æœéœ€è¦æ˜¾ç¤º token æ–‡æœ¬
        if show_tokens and len(tokens) <= 100:
            # æ˜¾ç¤ºéƒ¨åˆ† token æ–‡æœ¬ï¼ˆé¿å…å¤ªå¯†é›†ï¼‰
            step = max(1, len(tokens) // 50)
            tick_positions = range(0, len(tokens), step)
            tick_labels = [tokens[i][:10].replace('\n', 'â†µ') for i in tick_positions]
            ax.set_xticks(tick_positions)
            ax.set_xticklabels(tick_labels, rotation=45, ha='right', fontsize=8)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        stats_text = (
            f"Mean Entropy: {np.mean(entropies):.4f}\n"
            f"Std Entropy: {np.std(entropies):.4f}\n"
            f"Max Entropy: {np.max(entropies):.4f}\n"
            f"Min Entropy: {np.min(entropies):.4f}"
        )
        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, 
               fontsize=10, verticalalignment='top',
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        # æ·»åŠ å„ phase çš„ç»Ÿè®¡ä¿¡æ¯
        phase_stats_lines = []
        for phase_name, ranges in phase_ranges.items():
            if not ranges:
                continue
            phase_entropies = []
            for start_idx, end_idx in ranges:
                phase_entropies.extend(entropies[start_idx:end_idx])
            if phase_entropies:
                avg_e = np.mean(phase_entropies)
                phase_stats_lines.append(f"{phase_name}: avg={avg_e:.4f} (n={len(phase_entropies)})")
        
        if phase_stats_lines:
            phase_stats_text = "Phase Stats:\n" + "\n".join(phase_stats_lines)
            ax.text(0.02, 0.75, phase_stats_text, transform=ax.transAxes,
                   fontsize=9, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            logger.info(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    def analyze_and_plot_case(
        self, 
        data: List[Dict], 
        case_idx: int = 0,
        save_path: str = None,
        figsize: Tuple[int, int] = (16, 8),
        show_tokens: bool = False
    ) -> Dict:
        """
        åˆ†æå¹¶ç»˜åˆ¶å•ä¸ª case çš„ç†µæ›²çº¿
        
        Args:
            data: æ•°æ®é›†
            case_idx: è¦åˆ†æçš„ case ç´¢å¼•
            save_path: å›¾ç‰‡ä¿å­˜è·¯å¾„
            figsize: å›¾ç‰‡å¤§å°
            show_tokens: æ˜¯å¦æ˜¾ç¤º token æ–‡æœ¬
            
        Returns:
            case_data: åˆ†æç»“æœ
        """
        if case_idx >= len(data):
            logger.error(f"Case ç´¢å¼• {case_idx} è¶…å‡ºèŒƒå›´ (å…± {len(data)} ä¸ªæ ·æœ¬)")
            return {}
        
        sample = data[case_idx]
        messages = sample.get('messages', [])
        
        if not messages:
            logger.error(f"Case {case_idx} æ²¡æœ‰ messages")
            return {}
        
        logger.info(f"å¼€å§‹åˆ†æ Case {case_idx}...")
        case_data = self.analyze_single_case(messages)
        
        if case_data['tokens']:
            self.plot_entropy_curve(case_data, case_idx, save_path, figsize, show_tokens)
        else:
            logger.warning(f"Case {case_idx} æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
        
        return case_data
    
    def interactive_case_viewer(self, data: List[Dict], start_idx: int = 0):
        """
        äº¤äº’å¼æŸ¥çœ‹å¤šä¸ª case
        """
        current_idx = start_idx
        
        while True:
            print(f"\n{'='*50}")
            print(f"å½“å‰ Case: {current_idx} / {len(data) - 1}")
            print(f"{'='*50}")
            
            # åˆ†æå¹¶æ˜¾ç¤ºå½“å‰ case
            case_data = self.analyze_and_plot_case(data, current_idx)
            
            if case_data.get('tokens'):
                print(f"\nğŸ“Š Token æ•°é‡: {len(case_data['tokens'])}")
                print(f"ğŸ“ˆ å¹³å‡ç†µ: {np.mean(case_data['entropies']):.4f}")
                
                for phase_name, ranges in case_data['phase_ranges'].items():
                    if ranges:
                        total_tokens = sum(end - start for start, end in ranges)
                        print(f"  â€¢ {phase_name}: {len(ranges)} ä¸ªåŒºå—, å…± {total_tokens} tokens")
            
            # ç”¨æˆ·è¾“å…¥
            print("\næ“ä½œ: [n]ä¸‹ä¸€ä¸ª [p]ä¸Šä¸€ä¸ª [g]è·³è½¬ [s]ä¿å­˜ [q]é€€å‡º")
            user_input = input("è¯·è¾“å…¥: ").strip().lower()
            
            if user_input == 'n':
                current_idx = min(current_idx + 1, len(data) - 1)
            elif user_input == 'p':
                current_idx = max(current_idx - 1, 0)
            elif user_input == 'g':
                try:
                    new_idx = int(input("è¾“å…¥ case ç´¢å¼•: "))
                    if 0 <= new_idx < len(data):
                        current_idx = new_idx
                    else:
                        print(f"ç´¢å¼•è¶…å‡ºèŒƒå›´ [0, {len(data) - 1}]")
                except ValueError:
                    print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
            elif user_input == 's':
                save_path = input("è¾“å…¥ä¿å­˜è·¯å¾„ (é»˜è®¤: entropy_case_{idx}.png): ").strip()
                if not save_path:
                    save_path = f"entropy_case_{current_idx}.png"
                self.plot_entropy_curve(case_data, current_idx, save_path)
            elif user_input == 'q':
                break
    
    def analyze_dataset(self, data: List[Dict], max_samples: int = None) -> Dict[str, PhaseEntropy]:
        """åˆ†ææ•´ä¸ªæ•°æ®é›†"""
        from tqdm import tqdm
        
        phase_stats = {
            'think': PhaseEntropy('Think Phase (æ€è€ƒé˜¶æ®µ)'),
            'tool_call': PhaseEntropy('Action Phase (å·¥å…·è°ƒç”¨)'),
            'tool_response': PhaseEntropy('Tool Response (å·¥å…·å“åº”)'),
            'answer': PhaseEntropy('Answer Phase (å›ç­”é˜¶æ®µ)'),
        }
        
        samples = data[:max_samples] if max_samples else data
        logger.info(f"å¼€å§‹åˆ†æ {len(samples)} ä¸ªæ ·æœ¬...")
        
        successful = 0
        for idx, sample in enumerate(tqdm(samples, desc="åˆ†æè¿›åº¦")):
            messages = sample.get('messages', [])
            if not messages:
                continue
            
            try:
                case_data = self.analyze_single_case(messages)
                
                if not case_data['tokens']:
                    continue
                
                entropies = case_data['entropies']
                phase_ranges = case_data['phase_ranges']
                
                has_data = False
                for phase_name, ranges in phase_ranges.items():
                    for start_idx, end_idx in ranges:
                        phase_entropies = entropies[start_idx:end_idx]
                        if phase_entropies:
                            phase_stats[phase_name].add_entropies(phase_entropies)
                            has_data = True
                
                if has_data:
                    successful += 1
                        
            except Exception as e:
                logger.warning(f"æ ·æœ¬ {idx} åˆ†æå‡ºé”™: {e}")
                import traceback
                logger.debug(traceback.format_exc())
                continue
        
        logger.info(f"æˆåŠŸåˆ†æ {successful}/{len(samples)} ä¸ªæ ·æœ¬")
        return phase_stats
    
    def print_results(self, phase_stats: Dict[str, PhaseEntropy]):
        """æ‰“å°ç»“æœ"""
        print("\n" + "=" * 70)
        print("            ç†µç»Ÿè®¡åˆ†æç»“æœ (Entropy Analysis Results)")
        print("=" * 70)
        
        for phase_name, stats in phase_stats.items():
            print(f"\nğŸ“Š ã€{stats.phase_name}ã€‘")
            print(f"   â”œâ”€ Tokenæ•°é‡:      {stats.token_count:,}")
            print(f"   â”œâ”€ å¹³å‡ç†µ (Avg):   {stats.average_entropy:.4f}")
            print(f"   â”œâ”€ æ ‡å‡†å·® (Std):   {stats.std_entropy:.4f}")
            if stats.entropies:
                print(f"   â”œâ”€ æœ€å°ç†µ (Min):   {min(stats.entropies):.4f}")
                print(f"   â”œâ”€ æœ€å¤§ç†µ (Max):   {max(stats.entropies):.4f}")
                print(f"   â””â”€ ä¸­ä½æ•° (Med):   {float(np.median(stats.entropies)):.4f}")
            else:
                print(f"   â””â”€ (æ— æ•°æ®)")
        
        print("\n" + "=" * 70)
    
    def save_results(self, phase_stats: Dict[str, PhaseEntropy], output_path: str):
        """ä¿å­˜ç»“æœåˆ° JSON"""
        results = {
            'summary': {},
            'details': {}
        }
        
        for phase_name, stats in phase_stats.items():
            results['details'][phase_name] = {
                'phase_name': stats.phase_name,
                'token_count': stats.token_count,
                'average_entropy': stats.average_entropy,
                'std_entropy': stats.std_entropy,
            }
            results['summary'][phase_name] = {
                'avg_entropy': stats.average_entropy,
                'token_count': stats.token_count
            }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def diagnose_data(input_path: str):
    """è¯Šæ–­æ•°æ®æ–‡ä»¶ä¸­çš„ phase åˆ†å¸ƒ"""
    print("\n" + "=" * 70)
    print("ğŸ” æ•°æ®è¯Šæ–­")
    print("=" * 70)
    
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\nğŸ“‚ æ–‡ä»¶: {input_path}")
    print(f"ğŸ“Š æ ·æœ¬æ€»æ•°: {len(data)}")
    
    phase_patterns = {
        'think': re.compile(r'<think>(.*?)</think>', re.DOTALL),
        'tool_call': re.compile(r'<tool_call>(.*?)</tool_call>', re.DOTALL),
        'tool_response': re.compile(r'<tool_response>(.*?)</tool_response>', re.DOTALL),
        'answer': re.compile(r'<answer>(.*?)</answer>', re.DOTALL),
    }
    
    # ç»Ÿè®¡å„ role å’Œ phase çš„åˆ†å¸ƒ
    role_counts = {}
    phase_by_role = {phase: {} for phase in phase_patterns.keys()}
    
    for sample_idx, sample in enumerate(data[:10]):  # åªçœ‹å‰10ä¸ªæ ·æœ¬
        messages = sample.get('messages', [])
        
        print(f"\n--- Sample {sample_idx} ({len(messages)} messages) ---")
        
        for msg_idx, msg in enumerate(messages):
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            
            role_counts[role] = role_counts.get(role, 0) + 1
            
            detected = []
            for phase_name, pattern in phase_patterns.items():
                matches = pattern.findall(content)
                if matches:
                    detected.append(f"{phase_name}({len(matches)})")
                    phase_by_role[phase_name][role] = phase_by_role[phase_name].get(role, 0) + len(matches)
            
            if detected or role == 'assistant':
                content_preview = content[:60].replace('\n', 'â†µ') if content else "(empty)"
                print(f"  [{msg_idx}] role={role:10s} phases={detected or 'none':30s} | {content_preview}...")
    
    print("\n" + "-" * 70)
    print("ğŸ“ˆ Phase åˆ†å¸ƒæŒ‰ Role ç»Ÿè®¡ (å‰10ä¸ªæ ·æœ¬):")
    for phase_name, role_dist in phase_by_role.items():
        print(f"  {phase_name:15s}: {role_dist}")
    
    print("\nğŸ’¡ å¦‚æœ tool_response åªå‡ºç°åœ¨é assistant roleï¼Œéœ€è¦è°ƒæ•´ä»£ç é€»è¾‘")
    print("=" * 70)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Agent ç†µç»Ÿè®¡åˆ†æå·¥å…· - Case by Case å¯è§†åŒ–')
    parser.add_argument('--input', '-i', type=str, help='è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', type=str, default='entropy_results.json', help='è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--base-url', type=str, default='http://localhost:6001', help='sglang æœåŠ¡åœ°å€')
    parser.add_argument('--limit', type=int, default=None, help='é™åˆ¶æ ·æœ¬æ•°é‡')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯• API è¿æ¥')
    parser.add_argument('--diagnose', action='store_true', help='è¯Šæ–­æ•°æ®æ–‡ä»¶ä¸­çš„ phase åˆ†å¸ƒ')
    parser.add_argument('--debug', action='store_true', help='æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯')
    
    # æ–°å¢çš„ case-by-case å¯è§†åŒ–å‚æ•°
    parser.add_argument('--case', '-c', type=int, default=0, help='è¦åˆ†æçš„å•ä¸ª case ç´¢å¼• (é»˜è®¤: 0)')
    parser.add_argument('--plot', '-p', action='store_true', help='ç»˜åˆ¶å•ä¸ª case çš„ç†µæ›²çº¿')
    parser.add_argument('--save-plot', type=str, default=None, help='ä¿å­˜ç†µæ›²çº¿å›¾ç‰‡çš„è·¯å¾„')
    parser.add_argument('--interactive', action='store_true', help='äº¤äº’å¼æŸ¥çœ‹å¤šä¸ª case')
    parser.add_argument('--show-tokens', action='store_true', help='åœ¨å›¾è¡¨ä¸­æ˜¾ç¤º token æ–‡æœ¬')
    parser.add_argument('--figsize', type=str, default='16,8', help='å›¾ç‰‡å¤§å°ï¼Œæ ¼å¼: "å®½,é«˜"')
    
    args = parser.parse_args()
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # è§£æ figsize
    try:
        figsize = tuple(map(int, args.figsize.split(',')))
    except:
        figsize = (16, 8)
    
    if args.diagnose and args.input:
        diagnose_data(args.input)
        return
    
    if args.test:
        analyzer = SGLangEntropyAnalyzer(args.base_url)
        result = analyzer.get_completion_with_logprobs("Hello", max_tokens=5)
        if result:
            print("âœ… API è¿æ¥æˆåŠŸ")
            print(f"   ç”Ÿæˆ: {result['text']}")
        else:
            print("âŒ API è¿æ¥å¤±è´¥")
        return
    
    if not args.input:
        print("ç”¨æ³•:")
        print("  è¯Šæ–­æ•°æ®:       python script.py --diagnose -i data.json")
        print("  ç»˜åˆ¶å•ä¸ª case:  python script.py -i data.json --plot --case 0")
        print("  ä¿å­˜å›¾ç‰‡:       python script.py -i data.json --plot --case 0 --save-plot output.png")
        print("  äº¤äº’å¼æŸ¥çœ‹:     python script.py -i data.json --interactive")
        print("  æ‰¹é‡åˆ†æ:       python script.py -i data.json --limit 10")
        return
    
    # åŠ è½½æ•°æ®
    with open(args.input, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    analyzer = SGLangEntropyAnalyzer(args.base_url)
    
    # Case-by-case å¯è§†åŒ–æ¨¡å¼
    if args.plot:
        case_data = analyzer.analyze_and_plot_case(
            data, 
            case_idx=args.case,
            save_path=args.save_plot,
            figsize=figsize,
            show_tokens=args.show_tokens
        )
        
        if case_data.get('tokens'):
            print(f"\nğŸ“Š Case {args.case} åˆ†æå®Œæˆ")
            print(f"   Token æ•°é‡: {len(case_data['tokens'])}")
            print(f"   å¹³å‡ç†µ: {np.mean(case_data['entropies']):.4f}")
            
            for phase_name, ranges in case_data['phase_ranges'].items():
                if ranges:
                    total_tokens = sum(end - start for start, end in ranges)
                    phase_entropies = []
                    for start_idx, end_idx in ranges:
                        phase_entropies.extend(case_data['entropies'][start_idx:end_idx])
                    if phase_entropies:
                        print(f"   â€¢ {phase_name}: {len(ranges)} ä¸ªåŒºå—, {total_tokens} tokens, avg_entropy={np.mean(phase_entropies):.4f}")
        return
    
    # äº¤äº’å¼æ¨¡å¼
    if args.interactive:
        analyzer.interactive_case_viewer(data, start_idx=args.case)
        return
    
    # æ‰¹é‡åˆ†ææ¨¡å¼
    phase_stats = analyzer.analyze_dataset(data, max_samples=args.limit)
    analyzer.print_results(phase_stats)
    analyzer.save_results(phase_stats, args.output)


if __name__ == '__main__':
    main()